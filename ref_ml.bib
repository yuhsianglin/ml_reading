# howard-ruder-2018-universal
@inproceedings{ulmfit,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}

# radford-etal-2018-improving
@article{gpt,
    title = "Improving Language Understanding by Generative Pre-Training",
    author = "Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever",
    year = "2018",
    url = "https://openai.com/blog/language-unsupervised/",
}

# NIPS2017_7181
@incollection{transformer,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{Zhu2020Incorporating,
title={Incorporating BERT into Neural Machine Translation},
author={Jinhua Zhu and Yingce Xia and Lijun Wu and Di He and Tao Qin and Wengang Zhou and Houqiang Li and Tieyan Liu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hyl7ygStwB}
}

@inproceedings{clinchant-etal-2019-use,
    title = "On the use of {BERT} for Neural Machine Translation",
    author = "Clinchant, Stephane  and
      Jung, Kweon Woo  and
      Nikoulina, Vassilina",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-5611",
    doi = "10.18653/v1/D19-5611",
    pages = "108--117",
    abstract = "Exploiting large pretrained models for various NMT tasks have gained a lot of visibility recently. In this work we study how BERT pretrained models could be exploited for supervised Neural Machine Translation. We compare various ways to integrate pretrained BERT model with NMT model and study the impact of the monolingual data used for BERT training on the final translation quality. We use WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian datasets for these experiments. In addition to standard task test set evaluation, we perform evaluation on out-of-domain test sets and noise injected test sets, in order to assess how BERT pretrained representations affect model robustness.",
}

@inproceedings{imamura-sumita-2019-recycling,
    title = "Recycling a Pre-trained {BERT} Encoder for Neural Machine Translation",
    author = "Imamura, Kenji  and
      Sumita, Eiichiro",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-5603",
    doi = "10.18653/v1/D19-5603",
    pages = "23--31",
    abstract = "In this paper, a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model is applied to Transformer-based neural machine translation (NMT). In contrast to monolingual tasks, the number of unlearned model parameters in an NMT decoder is as huge as the number of learned parameters in the BERT model. To train all the models appropriately, we employ two-stage optimization, which first trains only the unlearned parameters by freezing the BERT model, and then fine-tunes all the sub-models. In our experiments, stable two-stage optimization was achieved, in contrast the BLEU scores of direct fine-tuning were extremely low. Consequently, the BLEU scores of the proposed method were better than those of the Transformer base model and the same model without pre-training. Additionally, we confirmed that NMT with the BERT encoder is more effective in low-resource settings.",
}

@inproceedings{liu-lapata-2019-text,
    title = "Text Summarization with Pretrained Encoders",
    author = "Liu, Yang  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1387",
    doi = "10.18653/v1/D19-1387",
    pages = "3730--3740",
    abstract = "Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.",
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@incollection{NIPS2019_9464,
title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {13063--13075},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9464-unified-language-model-pre-training-for-natural-language-understanding-and-generation.pdf}
}

@inproceedings{pbg,
  title={{PyTorch-BigGraph: A Large-scale Graph Embedding System}},
  author={Lerer, Adam and Wu, Ledell and Shen, Jiajun and Lacroix, Timothee and Wehrstedt, Luca and Bose, Abhijit and Peysakhovich, Alex},
  booktitle={Proceedings of the 2nd SysML Conference},
  year={2019},
  address={Palo Alto, CA, USA}
}

@inproceedings{velickovic2018graph,
title={Graph Attention Networks},
author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJXMpikCZ},
}

@incollection{NIPS2017_7213,
title = {Poincar\'{e} Embeddings for Learning Hierarchical Representations},
author = {Nickel, Maximillian and Kiela, Douwe},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6338--6347},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf}
}

@inproceedings{10.1145/3097983.3098061,
author = {Ribeiro, Leonardo F.R. and Saverese, Pedro H.P. and Figueiredo, Daniel R.},
title = {Struc2vec: Learning Node Representations from Structural Identity},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098061},
doi = {10.1145/3097983.3098061},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {385–394},
numpages = {10},
keywords = {feature learning, node embeddings, structural identity},
location = {Halifax, NS, Canada},
series = {KDD ’17}
}

@article{GOYAL201878,
title = "Graph embedding techniques, applications, and performance: A survey",
journal = "Knowledge-Based Systems",
volume = "151",
pages = "78 - 94",
year = "2018",
issn = "0950-7051",
doi = "https://doi.org/10.1016/j.knosys.2018.03.022",
url = "http://www.sciencedirect.com/science/article/pii/S0950705118301540",
author = "Palash Goyal and Emilio Ferrara",
keywords = "Graph embedding techniques, Graph embedding applications, Python graph embedding methods GEM library",
abstract = "Graphs, such as social networks, word co-occurrence networks, and communication networks, occur naturally in various real-world applications. Analyzing them yields insight into the structure of society, language, and different patterns of communication. Many approaches have been proposed to perform the analysis. Recently, methods which use the representation of graph nodes in vector space have gained traction from the research community. In this survey, we provide a comprehensive and structured analysis of various graph embedding techniques proposed in the literature. We first introduce the embedding task and its challenges such as scalability, choice of dimensionality, and features to be preserved, and their possible solutions. We then present three categories of approaches based on factorization methods, random walks, and deep learning, with examples of representative algorithms in each category and analysis of their performance on various tasks. We evaluate these state-of-the-art methods on a few common datasets and compare their performance against one another. Our analysis concludes by suggesting some potential applications and future directions. We finally present the open-source Python library we developed, named GEM (Graph Embedding Methods, available at https://github.com/palash1992/GEM), which provides all presented algorithms within a unified interface to foster and facilitate research on the topic."
}

@InProceedings{pmlr-v97-wu19e,
  title =    {Simplifying Graph Convolutional Networks},
  author =   {Wu, Felix and Souza, Amauri and Zhang, Tianyi and Fifty, Christopher and Yu, Tao and Weinberger, Kilian},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  pages =    {6861--6871},
  year =     {2019},
  editor =   {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =   {97},
  series =   {Proceedings of Machine Learning Research},
  address =      {Long Beach, California, USA},
  month =    {09--15 Jun},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v97/wu19e/wu19e.pdf},
  url =      {http://proceedings.mlr.press/v97/wu19e.html},
  abstract =     {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.}
}

@INPROCEEDINGS{8439897,  author={F. {Monti} and K. {Otness} and M. M. {Bronstein}},  booktitle={2018 IEEE Data Science Workshop (DSW)},   title={MOTIFNET: A MOTIF-BASED GRAPH CONVOLUTIONAL NETWORK FOR DIRECTED GRAPHS},   year={2018},  volume={},  number={},  pages={225-228},}

@incollection{NIPS2017_6703,
title = {Inductive Representation Learning on Large Graphs},
author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {1024--1034},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf}
}

@ARTICLE{8521593,  author={R. {Levie} and F. {Monti} and X. {Bresson} and M. M. {Bronstein}},  journal={IEEE Transactions on Signal Processing},   title={CayleyNets: Graph Convolutional Neural Networks With Complex Rational Spectral Filters},   year={2019},  volume={67},  number={1},  pages={97-109},}

@INPROCEEDINGS{8100059,  author={F. {Monti} and D. {Boscaini} and J. {Masci} and E. {Rodolà} and J. {Svoboda} and M. M. {Bronstein}},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs},   year={2017},  volume={},  number={},  pages={5425-5434},}

@inproceedings{kipf2017semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N. and Welling, Max},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@incollection{NIPS2016_6081,
title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
author = {Defferrard, Micha\"{e}l and Bresson, Xavier and Vandergheynst, Pierre},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3844--3852},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf}
}

@inproceedings{harp,
    title={HARP: Hierarchical Representation Learning for Networks},
    author={Chen, Haochen and Perozzi, Bryan and Hu, Yifan and Skiena, Steven},
    booktitle={Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence},
    year={2018},
    organization={AAAI Press}
}

@inproceedings{10.1145/2736277.2741093,
author = {Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
title = {LINE: Large-Scale Information Network Embedding},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741093},
doi = {10.1145/2736277.2741093},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1067–1077},
numpages = {11},
keywords = {information network embedding, dimension reduction, scalability, feature learning},
location = {Florence, Italy},
series = {WWW ’15}
}

@inproceedings{10.1145/2939672.2939753,
author = {Wang, Daixin and Cui, Peng and Zhu, Wenwu},
title = {Structural Deep Network Embedding},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939753},
doi = {10.1145/2939672.2939753},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1225–1234},
numpages = {10},
keywords = {deep learning, network embedding, network analysis},
location = {San Francisco, California, USA},
series = {KDD ’16}
}
  
@article{Narayanan2017graph2vecLD,
  title={graph2vec: Learning Distributed Representations of Graphs},
  author={Annamalai Narayanan and Mahinthan Chandramohan and Rajasekar Venkatesan and Lihui Chen and Yang P. Liu and Shantanu Jaiswal},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.05005}
}

@inproceedings{10.1145/2939672.2939754,
author = {Grover, Aditya and Leskovec, Jure},
title = {Node2vec: Scalable Feature Learning for Networks},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939754},
doi = {10.1145/2939672.2939754},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {855–864},
numpages = {10},
keywords = {graph representations, node embeddings, feature learning, information networks},
location = {San Francisco, California, USA},
series = {KDD ’16}
}

@inproceedings{10.1145/2623330.2623732,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {DeepWalk: Online Learning of Social Representations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623732},
doi = {10.1145/2623330.2623732},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {701–710},
numpages = {10},
keywords = {learning with partial labels, network classification, deep learning, social networks, latent representations, online learning},
location = {New York, New York, USA},
series = {KDD ’14}
}

@incollection{NIPS2018_8131,
title = {Watch Your Step: Learning Node Embeddings via Graph Attention},
author = {Abu-El-Haija, Sami and Perozzi, Bryan and Al-Rfou, Rami and Alemi, Alexander A},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {9180--9190},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8131-watch-your-step-learning-node-embeddings-via-graph-attention.pdf}
}

@inproceedings{10.1145/3308558.3313660,
author = {Epasto, Alessandro and Perozzi, Bryan},
title = {Is a Single Embedding Enough? Learning Node Representations That Capture Multiple Social Contexts},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313660},
doi = {10.1145/3308558.3313660},
booktitle = {The World Wide Web Conference},
pages = {394–404},
numpages = {11},
keywords = {graph embeddings, polysemous representations, representation learning},
location = {San Francisco, CA, USA},
series = {WWW ’19}
}

@inproceedings{10.1145/3097983.3098054,
author = {Epasto, Alessandro and Lattanzi, Silvio and Paes Leme, Renato},
title = {Ego-Splitting Framework: From Non-Overlapping to Overlapping Clusters},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098054},
doi = {10.1145/3097983.3098054},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {145–154},
numpages = {10},
keywords = {ego-nets, large-scale graph algorithms, overlapping clustering},
location = {Halifax, NS, Canada},
series = {KDD ’17}
}

@inproceedings{10.5555/3045390.3045396,
author = {Yang, Zhilin and Cohen, William W. and Salakhutdinov, Ruslan},
title = {Revisiting Semi-Supervised Learning with Graph Embeddings},
year = {2016},
publisher = {JMLR.org},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {40–48},
numpages = {9},
location = {New York, NY, USA},
series = {ICML’16}
}

@article{Sen_Namata_Bilgic_Getoor_Galligher_Eliassi-Rad_2008, title={Collective Classification in Network Data}, volume={29}, url={https://www.aaai.org/ojs/index.php/aimagazine/article/view/2157}, DOI={10.1609/aimag.v29i3.2157}, abstractNote={Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.}, number={3}, journal={AI Magazine}, author={Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Galligher, Brian and Eliassi-Rad, Tina}, year={2008}, month={Sep.}, pages={93}}

@inproceedings{bing-etal-2015-improving,
    title = "Improving Distant Supervision for Information Extraction Using Label Propagation Through Lists",
    author = "Bing, Lidong  and
      Chaudhari, Sneha  and
      Wang, Richard  and
      Cohen, William",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1060",
    doi = "10.18653/v1/D15-1060",
    pages = "524--529",
}

@inproceedings{10.1145/3219819.3219885,
author = {Grbovic, Mihajlo and Cheng, Haibin},
title = {Real-Time Personalization Using Embeddings for Search Ranking at Airbnb},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219885},
doi = {10.1145/3219819.3219885},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
pages = {311–320},
numpages = {10},
keywords = {search ranking, personalization, user modeling},
location = {London, United Kingdom},
series = {KDD ’18}
}

@inproceedings{DBLP:conf/www/ZhangWZ19,
  author    = {Yuan Zhang and
               Dong Wang and
               Yan Zhang},
  editor    = {Ling Liu and
               Ryen W. White and
               Amin Mantrach and
               Fabrizio Silvestri and
               Julian J. McAuley and
               Ricardo Baeza{-}Yates and
               Leila Zia},
  title     = {Neural {IR} Meets Graph Embedding: {A} Ranking Model for Product Search},
  booktitle = {The World Wide Web Conference, {WWW} 2019, San Francisco, CA, USA,
               May 13-17, 2019},
  pages     = {2390--2400},
  publisher = {{ACM}},
  year      = {2019},
  url       = {https://doi.org/10.1145/3308558.3313468},
  doi       = {10.1145/3308558.3313468},
  timestamp = {Sun, 22 Sep 2019 18:12:47 +0200},
  biburl    = {https://dblp.org/rec/conf/www/ZhangWZ19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3219819.3219890,
author = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219890},
doi = {10.1145/3219819.3219890},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
pages = {974–983},
numpages = {10},
keywords = {scalability, deep learning, graph convolutional networks, recommender systems},
location = {London, United Kingdom},
series = {KDD ’18}
}

@inproceedings{10.5555/3045390.3045609,
author = {Trouillon, Th\'{e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, \'{E}ric and Bouchard, Guillaume},
title = {Complex Embeddings for Simple Link Prediction},
year = {2016},
publisher = {JMLR.org},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {2071–2080},
numpages = {10},
location = {New York, NY, USA},
series = {ICML’16}
}

@inproceedings{10.5555/3104482.3104584,
author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
title = {A Three-Way Model for Collective Learning on Multi-Relational Data},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {809–816},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML’11}
}

@Article{mitra2018an,
author = {Mitra, Bhaskar and Craswell, Nick},
title = {An Introduction to Neural Information Retrieval},
year = {2018},
month = {December},
abstract = {Neural ranking models for information retrieval (IR) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ supervised machine learning (ML) techniques—including neural networks—over hand-crafted IR features. By contrast, more recently proposed neural models learn representations of language from raw text that can bridge the gap between query and document vocabulary. Unlike classical learning to rank models and non-neural approaches to IR, these new ML techniques are data-hungry, requiring large scale training data before they can be deployed. This tutorial introduces basic concepts and intuitions behind neural IR models, and places them in the context of classical non-neural approaches to IR. We begin by introducing fundamental concepts of retrieval and different neural and non-neural approaches to unsupervised learning of vector representations of text. We then review IR methods that employ these pre-trained neural vector representations without learning the IR task end-to-end. We introduce the Learning to Rank (LTR) framework next, discussing standard loss functions for ranking. We follow that with an overview of deep neural networks (DNNs), including standard architectures and implementations. Finally, we review supervised neural learning to rank models, including recent DNN architectures trained end-to-end for ranking tasks. We conclude with a discussion on potential future directions for neural IR.},
url = {https://www.microsoft.com/en-us/research/publication/introduction-neural-information-retrieval/},
pages = {1-126},
journal = {Foundations and Trends® in Information Retrieval},
volume = {13},
number = {1},
}

@InProceedings{huang2013learning,
author = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
title = {Learning Deep Structured Semantic Models for Web Search using Clickthrough Data},
year = {2013},
month = {October},
abstract = {Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.},
publisher = {ACM International Conference on Information and Knowledge Management (CIKM)},
url = {https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/},
}

@inproceedings{Zamani2018,
author = {Zamani, Hamed and Mitra, Bhaskar and Song, Xia and Craswell, Nick and Tiwary, Saurabh},
title = {Neural Ranking Models with Multiple Document Fields},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3159730},
doi = {10.1145/3159652.3159730},
abstract = {Deep neural networks have recently shown promise in the ad-hoc retrieval task. However, such models have often been based on one field of the document, for example considering document title only or document body only. Since in practice documents typically have multiple fields, and given that non-neural ranking models such as BM25F have been developed to take advantage of document structure, this paper investigates how neural models can deal with multiple document fields. We introduce a model that can consume short text fields such as document title and long text fields such as document body. It can also handle multi-instance fields with variable number of instances, for example where each document has zero or more instances of incoming anchor text. Since fields vary in coverage and quality, we introduce a masking method to handle missing field instances, as well as a field-level dropout method to avoid relying too much on any one field. As in the studies of non-neural field weighting, we find it is better for the ranker to score the whole document jointly, rather than generate a per-field score and aggregate. We find that different document fields may match different aspects of the query and therefore benefit from comparing with separate representations of the query text. The combination of techniques introduced here leads to a neural ranker that can take advantage of full document structure, including multiple instance and missing instance data, of variable length. The techniques significantly enhance the performance of the ranker, and outperform a learning to rank baseline with hand-crafted features.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {700–708},
numpages = {9},
keywords = {document representation, deep neural networks, structured documents, representation learning, neural ranking models},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@inproceedings{He2017,
author = {He, Xiangnan and Chua, Tat-Seng},
title = {Neural Factorization Machines for Sparse Predictive Analytics},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080777},
doi = {10.1145/3077136.3080777},
abstract = {Many predictive tasks of web applications need to model categorical variables, such as user IDs and demographics like genders and occupations. To apply standard machine learning techniques, these categorical predictors are always converted to a set of binary features via one-hot encoding, making the resultant feature vector highly sparse. To learn from such sparse data effectively, it is crucial to account for the interactions between features.Factorization Machines (FMs) are a popular solution for efficiently using the second-order feature interactions. However, FM models feature interactions in a linear way, which can be insufficient for capturing the non-linear and complex inherent structure of real-world data. While deep neural networks have recently been applied to learn non-linear feature interactions in industry, such as the Wide&amp;Deep by Google and DeepCross by Microsoft, the deep structure meanwhile makes them difficult to train.In this paper, we propose a novel model Neural Factorization Machine (NFM) for prediction under sparse settings. NFM seamlessly combines the linearity of FM in modelling second-order feature interactions and the non-linearity of neural network in modelling higher-order feature interactions. Conceptually, NFM is more expressive than FM since FM can be seen as a special case of NFM without hidden layers. Empirical results on two regression tasks show that with one hidden layer only, NFM significantly outperforms FM with a 7.3% relative improvement. Compared to the recent deep learning methods Wide&amp;Deep and DeepCross, our NFM uses a shallower structure but offers better performance, being much easier to train and tune in practice.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {355–364},
numpages = {10},
keywords = {sparse data, recommendation, regression, deep learning, neural networks, factorization machines},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@article{Zhang2019,
author = {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
title = {Deep Learning Based Recommender System: A Survey and New Perspectives},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3285029},
doi = {10.1145/3285029},
abstract = {With the growing volume of online information, recommender systems have been an effective strategy to overcome information overload. The utility of recommender systems cannot be overstated, given their widespread adoption in many web applications, along with their potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also to the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. The field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning-based recommender systems. More concretely, we provide and devise a taxonomy of deep learning-based recommendation models, along with a comprehensive summary of the state of the art. Finally, we expand on current trends and provide new perspectives pertaining to this new and exciting development of the field.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {5},
numpages = {38},
keywords = {survey, Recommender system, deep learning}
}

@INPROCEEDINGS{7837964,  author={Y. {Qu} and H. {Cai} and K. {Ren} and W. {Zhang} and Y. {Yu} and Y. {Wen} and J. {Wang}},  booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)},   title={Product-Based Neural Networks for User Response Prediction},   year={2016},  volume={},  number={},  pages={1149-1154},  doi={10.1109/ICDM.2016.0151}}

@article{45413,
title = {Wide & Deep Learning for Recommender Systems},
author  = {Heng-Tze Cheng and Levent Koc and Jeremiah Harmsen and Tal Shaked and Tushar Chandra and Hrishi Aradhye and Glen Anderson and Greg Corrado and Wei Chai and Mustafa Ispir and Rohan Anil and Zakaria Haque and Lichan Hong and Vihan Jain and Xiaobing Liu and Hemal Shah},
year  = {2016},
URL = {http://arxiv.org/abs/1606.07792},
journal = {arXiv:1606.07792}
}

@inproceedings{ijcai2017-0239,
  author    = {Huifeng Guo and Ruiming TANG and Yunming Ye and Zhenguo Li and Xiuqiang He},
  title     = {DeepFM: A Factorization-Machine based Neural Network for CTR Prediction},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {1725--1731},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/239},
  url       = {https://doi.org/10.24963/ijcai.2017/239},
}

@inproceedings{10.1145/3219819.3220023,
author = {Lian, Jianxun and Zhou, Xiaohuan and Zhang, Fuzheng and Chen, Zhongxia and Xie, Xing and Sun, Guangzhong},
title = {XDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220023},
doi = {10.1145/3219819.3220023},
abstract = {Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at https://github.com/Leavingseason/xDeepFM.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1754–1763},
numpages = {10},
keywords = {feature interactions, neural network, deep learning, recommender systems, factorization machines},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3357384.3357925,
author = {Song, Weiping and Shi, Chence and Xiao, Zhiping and Duan, Zhijian and Xu, Yewen and Zhang, Ming and Tang, Jian},
title = {AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357925},
doi = {10.1145/3357384.3357925},
abstract = {Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on high-order combinatorial features (a.k.a. cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding low-dimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the AutoInt to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multi-head self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the low-dimensional space. With different layers of the multi-head self-attentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: urlhttps://github.com/DeepGraphLearning/RecommenderSystems.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1161–1170},
numpages = {10},
keywords = {ctr prediction, self attention, high-order feature interactions, explainable recommendation},
location = {Beijing, China},
series = {CIKM '19}
}

@InProceedings{pmlr-v70-finn17a, title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}, author = {Chelsea Finn and Pieter Abbeel and Sergey Levine}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {1126--1135}, year = {2017}, editor = {Doina Precup and Yee Whye Teh}, volume = {70}, series = {Proceedings of Machine Learning Research}, address = {International Convention Centre, Sydney, Australia}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf}, url = {http://proceedings.mlr.press/v70/finn17a.html}, abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.} }

@article {Lake1332,
    author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
    title = {Human-level concept learning through probabilistic program induction},
    volume = {350},
    number = {6266},
    pages = {1332--1338},
    year = {2015},
    doi = {10.1126/science.aab3050},
    publisher = {American Association for the Advancement of Science},
    abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look {\textquotedblleft}right{\textquotedblright} as judged by Turing-like tests of the model{\textquoteright}s output in comparison to what real humans produce.Science, this issue p. 1332People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms{\textemdash}for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world{\textquoteright}s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several {\textquotedblleft}visual Turing tests{\textquotedblright} probing the model{\textquoteright}s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
    issn = {0036-8075},
    URL = {https://science.sciencemag.org/content/350/6266/1332},
    eprint = {https://science.sciencemag.org/content/350/6266/1332.full.pdf},
    journal = {Science}
}

@inproceedings{NIPS2016_90e13578,
 author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and kavukcuoglu, koray and Wierstra, Daan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Matching Networks for One Shot Learning},
 url = {https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf},
 volume = {29},
 year = {2016}
}

@InProceedings{pmlr-v48-santoro16, title = {Meta-Learning with Memory-Augmented Neural Networks}, author = {Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy Lillicrap}, booktitle = {Proceedings of The 33rd International Conference on Machine Learning}, pages = {1842--1850}, year = {2016}, editor = {Maria Florina Balcan and Kilian Q. Weinberger}, volume = {48}, series = {Proceedings of Machine Learning Research}, address = {New York, New York, USA}, month = {20--22 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v48/santoro16.pdf}, url = {http://proceedings.mlr.press/v48/santoro16.html}, abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.} }

@inproceedings{10.1145/3331184.3331268,
author = {Pan, Feiyang and Li, Shuokai and Ao, Xiang and Tang, Pingzhong and He, Qing},
title = {Warm Up Cold-Start Advertisements: Improving CTR Predictions via Learning to Learn ID Embeddings},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331268},
doi = {10.1145/3331184.3331268},
abstract = {Click-through rate (CTR) prediction has been one of the most central problems in computational advertising. Lately, embedding techniques that produce low-dimensional representations of ad IDs drastically improve CTR prediction accuracies. However, such learning techniques are data demanding and work poorly on new ads with little logging data, which is known as the cold-start problem.In this paper, we aim to improve CTR predictions during both the cold-start phase and the warm-up phase when a new ad is added to the candidate pool. We propose Meta-Embedding, a meta-learning-based approach that learns to generate desirable initial embeddings for new ad IDs. The proposed method trains an embedding generator for new ad IDs by making use of previously learned ads through gradient-based meta-learning. In other words, our method learns how to learn better embeddings. When a new ad comes, the trained generator initializes the embedding of its ID by feeding its contents and attributes. Next, the generated embedding can speed up the model fitting during the warm-up phase when a few labeled examples are available, compared to the existing initialization methods.Experimental results on three real-world datasets showed that Meta-Embedding can significantly improve both the cold-start and warm-up performances for six existing CTR prediction models, ranging from lightweight models such as Factorization Machines to complicated deep models such as PNN and DeepFM. All of the above apply to conversion rate (CVR) predictions as well.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {695–704},
numpages = {10},
keywords = {online advertising, meta-learning, ctr prediction, cold-start},
location = {Paris, France},
series = {SIGIR'19}
}

@misc{nakkiran2021deep,
      title={The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers}, 
      author={Preetum Nakkiran and Behnam Neyshabur and Hanie Sedghi},
      year={2021},
      eprint={2010.08127},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Dacrema2019,
author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
title = {Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3347058},
doi = {10.1145/3298689.3347058},
abstract = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models.In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {101–109},
numpages = {9},
keywords = {deep learning, evaluation, reproducibility, recommender systems},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{Ng2018,
    title={Machine Learning Yearning},
    author={Andrew Ng},
    publisher={Andrew Ng},
    note={\url{https://github.com/ajaymache/machine-learning-yearning}},
    year={2018}
}

@book{Burkov2020,
    title={Machine Learning Engineering},
    author={Andriy Burkov},
    publisher={Andriy Burkov},
    note={\url{http://www.mlebook.com/wiki/doku.php}},
    year={2020}
}

@book{Valiant2013,
    title={Probably Approximately Correct},
    author={Valiant, Leslie},
    publisher={Basic Books},
    year={2013}
}

@book{Burkov2019,
    title={The Hundred-Page Machine Learning Book},
    author={Andriy Burkov},
    publisher={Andriy Burkov},
    note={\url{http://themlbook.com/}},
    year={2019}
}

@book{Zhang2021,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C.~Lipton and Mu Li and and Alexander J.~Smola},
    note={\url{https://d2l.ai/}},
    year={2021}
}

@book{Lapan2018,
    title={Deep Reinforcement Learning Hands-On},
    author={Maxim Lapan},
    publisher={Packt},
    note={\url{https://www.packtpub.com/product/deep-reinforcement-learning-hands-on/9781788834247}},
    year={2018}
}

@book{Sutton2018,
    title={Reinforcement Learning: An Introduction},
    author={Richard S.~Sutton and Andrew G.~Barto},
    publisher={Bradford Books},
    year={2018}
}

@book{Pearl2018,
    title={The Book of Why: The New Science of Cause and Effect},
    author={Judea Pearl and Dana Mackenzie},
    publisher={Basic Books},
    year={2018}
}

@inproceedings{10.1145/2740908.2742726,
author = {Sedhain, Suvash and Menon, Aditya Krishna and Sanner, Scott and Xie, Lexing},
title = {AutoRec: Autoencoders Meet Collaborative Filtering},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742726},
doi = {10.1145/2740908.2742726},
abstract = {This paper proposes AutoRec, a novel autoencoder framework for collaborative filtering (CF). Empirically, AutoRec's compact and efficiently trainable model outperforms state-of-the-art CF techniques (biased matrix factorization, RBM-CF and LLORMA) on the Movielens and Netflix datasets.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {111–112},
numpages = {2},
keywords = {recommender systems, collaborative filtering, autoencoders},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3038912.3052569,
author = {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
title = {Neural Collaborative Filtering},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052569},
doi = {10.1145/3038912.3052569},
abstract = {In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation --- collaborative filtering --- on the basis of implicit feedback.Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering --- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items.By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {173–182},
numpages = {10},
keywords = {neural networks, implicit feedback, collaborative filtering, matrix factorization, deep learning},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1109/ICDM.2010.127,
author = {Rendle, Steffen},
title = {Factorization Machines},
year = {2010},
isbn = {9780769542560},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDM.2010.127},
doi = {10.1109/ICDM.2010.127},
abstract = {In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models.},
booktitle = {Proceedings of the 2010 IEEE International Conference on Data Mining},
pages = {995–1000},
numpages = {6},
keywords = {sparse data, tensor factorization, factorization machine, support vector machine},
series = {ICDM '10}
}
